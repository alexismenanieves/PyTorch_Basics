{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Suppose you have a 28x28 set of grayscale images, and you're assigned to \n",
    "classify whether it's certain class or not, considering there are 10 \n",
    "classes. You can try a neural network for this task, so you devise the \n",
    "following architecture:  \n",
    "\n",
    "![image1.png](../images/image1.png)  \n",
    "\n",
    "Each node correspond to a pixel of the image. Hidden layers have activation \n",
    "functions defined (in this case, ReLU), and the output layer has a final \n",
    "decision layer (LogSoftmax) for the ten classes it has.\n",
    "\n",
    "We could try another architecture with each hidden layer having 784 nodes, \n",
    "that it, the same quantity that the input layer. So each node is connected \n",
    "to the other nodes, and that connection is called weight. What we want is \n",
    "to find the weight that minimizes the prediction error. So we train our \n",
    "neural network and we can calculate how many parameters (the weights) we \n",
    "will need. For this case:\n",
    "\n",
    "![image2.png](../images/image2.png)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X[N,C,H,W]: torch.Size([64, 1, 28, 28]), type: torch.float32\n",
      "Size of y: torch.Size([64]), type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# DataLoader wraps an iterable over our dataset, and supports automatic \n",
    "# batching, sampling, shuffling and multiprocess data loading\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    # N: Batch size, C: Channels, H: Height, W: Width\n",
    "    print(f'Size of X[N,C,H,W]: {X.shape}, type: {X.dtype}')\n",
    "    print(f'Size of y: {y.shape}, type: {y.dtype}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "print(f'{mps_device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(mps_device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(mps_device), y.to(mps_device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1) * len(X)\n",
    "            print(f'Loss: {loss:>.8f}, [{(current / size)}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches =  len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(mps_device), y.to(mps_device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Test error: \\n Accuracy {correct*100:>.1f}% ,avg loss: {test_loss:>.8f}')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "Loss: 2.30007744, [0.0010666666666666667]\n",
      "Loss: 0.62993050, [0.10773333333333333]\n",
      "Loss: 0.36848697, [0.2144]\n",
      "Loss: 0.54830742, [0.32106666666666667]\n",
      "Loss: 0.57817626, [0.42773333333333335]\n",
      "Loss: 0.46647772, [0.5344]\n",
      "Loss: 0.42265308, [0.6410666666666667]\n",
      "Loss: 0.53752494, [0.7477333333333334]\n",
      "Loss: 0.53509820, [0.8544]\n",
      "Loss: 0.53787357, [0.9610666666666666]\n",
      "Test error: \n",
      " Accuracy 84.4% ,avg loss: 0.42449598\n",
      "Epoch 2\n",
      "\n",
      "Loss: 0.28726852, [0.0010666666666666667]\n",
      "Loss: 0.37831783, [0.10773333333333333]\n",
      "Loss: 0.29176784, [0.2144]\n",
      "Loss: 0.47445440, [0.32106666666666667]\n",
      "Loss: 0.41815668, [0.42773333333333335]\n",
      "Loss: 0.44904909, [0.5344]\n",
      "Loss: 0.36283976, [0.6410666666666667]\n",
      "Loss: 0.47039002, [0.7477333333333334]\n",
      "Loss: 0.40545613, [0.8544]\n",
      "Loss: 0.51748407, [0.9610666666666666]\n",
      "Test error: \n",
      " Accuracy 86.0% ,avg loss: 0.39232450\n",
      "Epoch 3\n",
      "\n",
      "Loss: 0.28840831, [0.0010666666666666667]\n",
      "Loss: 0.29506528, [0.10773333333333333]\n",
      "Loss: 0.25237128, [0.2144]\n",
      "Loss: 0.35961080, [0.32106666666666667]\n",
      "Loss: 0.44006202, [0.42773333333333335]\n",
      "Loss: 0.39461648, [0.5344]\n",
      "Loss: 0.28841415, [0.6410666666666667]\n",
      "Loss: 0.52494925, [0.7477333333333334]\n",
      "Loss: 0.44486496, [0.8544]\n",
      "Loss: 0.40406752, [0.9610666666666666]\n",
      "Test error: \n",
      " Accuracy 86.2% ,avg loss: 0.37302899\n",
      "Epoch 4\n",
      "\n",
      "Loss: 0.23569235, [0.0010666666666666667]\n",
      "Loss: 0.34218168, [0.10773333333333333]\n",
      "Loss: 0.22220317, [0.2144]\n",
      "Loss: 0.40926600, [0.32106666666666667]\n",
      "Loss: 0.40939504, [0.42773333333333335]\n",
      "Loss: 0.30353242, [0.5344]\n",
      "Loss: 0.36444226, [0.6410666666666667]\n",
      "Loss: 0.52752078, [0.7477333333333334]\n",
      "Loss: 0.35449123, [0.8544]\n",
      "Loss: 0.42501351, [0.9610666666666666]\n",
      "Test error: \n",
      " Accuracy 86.8% ,avg loss: 0.35326554\n",
      "Epoch 5\n",
      "\n",
      "Loss: 0.21399619, [0.0010666666666666667]\n",
      "Loss: 0.35336775, [0.10773333333333333]\n",
      "Loss: 0.23325059, [0.2144]\n",
      "Loss: 0.37082213, [0.32106666666666667]\n",
      "Loss: 0.31692839, [0.42773333333333335]\n",
      "Loss: 0.35815132, [0.5344]\n",
      "Loss: 0.23766139, [0.6410666666666667]\n",
      "Loss: 0.46915728, [0.7477333333333334]\n",
      "Loss: 0.39578649, [0.8544]\n",
      "Loss: 0.45869654, [0.9610666666666666]\n",
      "Test error: \n",
      " Accuracy 87.3% ,avg loss: 0.35439750\n",
      "Epoch 6\n",
      "\n",
      "Loss: 0.19817299, [0.0010666666666666667]\n",
      "Loss: 0.32111374, [0.10773333333333333]\n",
      "Loss: 0.20239645, [0.2144]\n",
      "Loss: 0.27112299, [0.32106666666666667]\n",
      "Loss: 0.45438546, [0.42773333333333335]\n",
      "Loss: 0.33604157, [0.5344]\n",
      "Loss: 0.23735586, [0.6410666666666667]\n",
      "Loss: 0.40058017, [0.7477333333333334]\n",
      "Loss: 0.46511692, [0.8544]\n",
      "Loss: 0.35862756, [0.9610666666666666]\n",
      "Test error: \n",
      " Accuracy 87.0% ,avg loss: 0.35571541\n",
      "Epoch 7\n",
      "\n",
      "Loss: 0.21207573, [0.0010666666666666667]\n",
      "Loss: 0.30738002, [0.10773333333333333]\n",
      "Loss: 0.20224291, [0.2144]\n",
      "Loss: 0.29431880, [0.32106666666666667]\n",
      "Loss: 0.37860906, [0.42773333333333335]\n",
      "Loss: 0.32991248, [0.5344]\n",
      "Loss: 0.28260249, [0.6410666666666667]\n",
      "Loss: 0.32523072, [0.7477333333333334]\n",
      "Loss: 0.35477763, [0.8544]\n",
      "Loss: 0.35699171, [0.9610666666666666]\n",
      "Test error: \n",
      " Accuracy 87.4% ,avg loss: 0.34020436\n",
      "Epoch 8\n",
      "\n",
      "Loss: 0.19864997, [0.0010666666666666667]\n",
      "Loss: 0.30270982, [0.10773333333333333]\n",
      "Loss: 0.22922879, [0.2144]\n",
      "Loss: 0.26526028, [0.32106666666666667]\n",
      "Loss: 0.27470815, [0.42773333333333335]\n",
      "Loss: 0.32058918, [0.5344]\n",
      "Loss: 0.24755850, [0.6410666666666667]\n",
      "Loss: 0.38939494, [0.7477333333333334]\n",
      "Loss: 0.26283506, [0.8544]\n",
      "Loss: 0.36487406, [0.9610666666666666]\n",
      "Test error: \n",
      " Accuracy 87.7% ,avg loss: 0.34802990\n",
      "Epoch 9\n",
      "\n",
      "Loss: 0.20486987, [0.0010666666666666667]\n",
      "Loss: 0.25532144, [0.10773333333333333]\n",
      "Loss: 0.20140609, [0.2144]\n",
      "Loss: 0.38098541, [0.32106666666666667]\n",
      "Loss: 0.30431718, [0.42773333333333335]\n",
      "Loss: 0.32450256, [0.5344]\n",
      "Loss: 0.26991901, [0.6410666666666667]\n",
      "Loss: 0.31304389, [0.7477333333333334]\n",
      "Loss: 0.31865835, [0.8544]\n",
      "Loss: 0.30683041, [0.9610666666666666]\n",
      "Test error: \n",
      " Accuracy 88.0% ,avg loss: 0.32691659\n",
      "Epoch 10\n",
      "\n",
      "Loss: 0.18797988, [0.0010666666666666667]\n",
      "Loss: 0.23577712, [0.10773333333333333]\n",
      "Loss: 0.24782440, [0.2144]\n",
      "Loss: 0.24512175, [0.32106666666666667]\n",
      "Loss: 0.35748428, [0.42773333333333335]\n",
      "Loss: 0.34715945, [0.5344]\n",
      "Loss: 0.21927893, [0.6410666666666667]\n",
      "Loss: 0.35841000, [0.7477333333333334]\n",
      "Loss: 0.29584509, [0.8544]\n",
      "Loss: 0.34791860, [0.9610666666666666]\n",
      "Test error: \n",
      " Accuracy 87.7% ,avg loss: 0.32867619\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n')\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog():\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "    def bark(self):\n",
    "        print(self.name + ' is barking')\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        print(f'{self.name} is {self.age} years old')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
